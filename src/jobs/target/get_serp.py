import os
import time
import argparse
from typing import List
from datetime import datetime
from utils.file import TXTFileHandler, JsonlFileHandler, GZipFileHandler
from utils.hdfs import HdfsFileHandler
from collector.serp_collector.serp_collector import SerpCollector
from utils.task_history import TaskHistory
from utils.slack import ds_trend_finder_dbgout, ds_trend_finder_dbgout_error
from utils.decorator import error_notifier
from config import postgres_db_config
from serp.serp_checker import SerpChecker, SerpCheckerKo, SerpCheckerJa, SerpCheckerEn

def get_keywords_already_collected_serp(
                                        lang:str, # ["ko", "ja"]
                                        date:str # yyyymmdd
                                        ) -> List[str]:
    '''
    hdfsì—ì„œ í•´ë‹¹ ë‚ ì§œì— ì„œí”„ê°€ ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    '''
    hdfs = HdfsFileHandler()
    services = ["google", 'youtube']
    suggest_types = ["basic", "target"]
    keywords = []
    for service in services:
        for suggest_type in suggest_types:
            file_path = f"/user/ds/wordpopcorn/{lang}/daily/{service}_suggest_for_llm_entity_topic/{date[:4]}/{date[:6]}/{date[:8]}/serp_keywords_{suggest_type}.txt"
            if not hdfs.exist(file_path):
                print(f"[{datetime.now()}] í•´ë‹¹ hdfs ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (file_path : {file_path})")
                continue
            contents = hdfs.load(file_path)
            keywords += list(set([line.strip() for line in contents.splitlines() if line.strip()]))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    return keywords

class EntitySerpDaily:

    def __init__(self, job_id:str, lang:str, service:str, log_task_history:bool=False):
        # ê¸°ë³¸ ì •ë³´
        self.job_id = job_id
        self.lang = lang
        self.service = service
        self.suggest_type = "target"
        self.serp_collector = SerpCollector(lang)

        # local ê´€ë ¨
        self.local_folder_path = f"./data/result/{self.suggest_type}/{self.service}/{self.lang}"
        if not os.path.exists(self.local_folder_path):
            os.makedirs(self.local_folder_path)
        self.suggest_completed_file = f"{self.local_folder_path}/{self.job_id}.jsonl.gz"
        self.trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords.txt"
        self.new_trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords_new.txt"
        self.serp_download_local_path = f"{self.local_folder_path}/{self.job_id}_serp.jsonl"
        self.serp_download_local_path_non_domestic = f"{self.local_folder_path}/{self.job_id}_serp_non_domestic.jsonl"
        
        # hdfs ê´€ë ¨
        self.hdfs = HdfsFileHandler()
        self.hdfs_upload_folder = f"/user/ds/wordpopcorn/{self.lang}/daily/{self.service}_suggest_for_llm_entity_topic/{self.job_id[:4]}/{self.job_id[:6]}/{self.job_id[:8]}/{self.job_id}"
        self.hdfs_already_collected_serp_keywords_path = f"/user/ds/wordpopcorn/{self.lang}/daily/{self.service}_suggest_for_llm_entity_topic/{self.job_id[:4]}/{self.job_id[:6]}/{self.job_id[:8]}/serp_keywords_{self.suggest_type}.txt"
        
        # log history ê´€ë ¨
        self.log_task_history = log_task_history
        self.task_name = f"ìˆ˜ì§‘-ì„œí”„-{service}-{self.suggest_type}"
        self.task_history = TaskHistory(postgres_db_config, "trend_finder", self.task_name, self.job_id, self.lang)

        # slack ê´€ë ¨
        self.slack_prefix_msg = f"Job Id : `{self.job_id}`\nTask Name : `{self.task_name}`-`{self.lang}`"

        # serp_checker ê´€ë ¨
        self.serp_checker = self.get_serp_checker()

    @error_notifier
    def append_keywords_to_serp_keywords_txt(self, keywords, log:bool=True):
        '''
        hdfsì— ì €ì¥ëœ serp_keywords_target.txtì— í‚¤ì›Œë“œ ì¶”ê°€
        '''
        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¤„ë°”ê¿ˆìœ¼ë¡œ ê° í‚¤ì›Œë“œ êµ¬ë¶„)
        keyword_data = '\n'.join(keywords) + '\n'

        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not self.hdfs.exist(self.hdfs_already_collected_serp_keywords_path):
            # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìƒˆë¡œ ìƒì„±
            self.hdfs.write(self.hdfs_already_collected_serp_keywords_path, keyword_data, encoding='utf-8')
        else:
            # íŒŒì¼ì´ ì¡´ì¬í•  ê²½ìš°, append ëª¨ë“œë¡œ í‚¤ì›Œë“œ ì¶”ê°€
            self.hdfs.write(self.hdfs_already_collected_serp_keywords_path, keyword_data, encoding='utf-8', append=True)
        if log:
            print(f"[{datetime.now()}] hdfsì— í‚¤ì›Œë“œ ì¶”ê°€ ì™„ë£Œ (keywords : {len(keywords)}ê°œ í‚¤ì›Œë“œ, hdfs_path : {self.hdfs_already_collected_serp_keywords_path})")

    @error_notifier
    def get_serp_checker(self) -> SerpChecker:
        if self.lang == "ko":
            return SerpCheckerKo
        elif self.lang == "ja":
            return SerpCheckerJa
        elif self.lang == "en":
            return SerpCheckerEn
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. (lang : {self.lang})")
        
    @error_notifier
    def collect_serp(self, keywords : List[str]):
        batch_size = 100
        error_keywords_len = 999

        while error_keywords_len > 0: # ì—ëŸ¬ í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œê¹Œì§€ ë°˜ë³µ
            error_keywords = []
            for i in range(0, len(keywords), batch_size):
                print(f"[{datetime.now()}] {i}/{len(keywords)}")
                res, error_res = self.serp_collector.get_serp_from_serp_api(keywords[i:i+batch_size], 
                                                                            domain="llm_entity_topic")
                domestic_serps = []
                non_domestic_serps = []
                for r in res:
                    if self.is_domestic_serp(r): domestic_serps.append(r)
                    else: non_domestic_serps.append(r)
                
                print(f"[{datetime.now()}] domestic_serps : {len(domestic_serps)}/{len(res)}ê°œ, non_domestic_serps : {len(non_domestic_serps)}/{len(res)}ê°œ")

                JsonlFileHandler(self.serp_download_local_path).write(domestic_serps)
                JsonlFileHandler(self.serp_download_local_path_non_domestic).write(non_domestic_serps)

                error_keywords += [r[0]['query'] for r in error_res if (len(r)>0 and type(r[0])==dict and 'query' in r[0])]
            error_keywords_len = len(error_keywords) # ì—ëŸ¬ í‚¤ì›Œë“œ ìˆ˜ ê°±ì‹ 
            keywords = error_keywords # ì—ëŸ¬ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ìˆ˜ì§‘
            print(f"[{datetime.now()}] ì„œí”„ ìˆ˜ì§‘ ì—ëŸ¬ í‚¤ì›Œë“œ : {error_keywords_len} ê°œ")
        print(f"[{datetime.now()}] ì„œí”„ ìˆ˜ì§‘ ì™„ë£Œ (local dest : {self.serp_download_local_path})")

    @error_notifier
    def is_domestic_serp(self, serp:dict) -> bool:
        return self.serp_checker(serp).is_domestic()

    @error_notifier
    def upload_to_hdfs(self):
        '''
        ê²°ê³¼ë¥¼ hdfsì— ì €ì¥
        '''
        try:
            if os.path.exists(self.serp_download_local_path):
                target_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_serp.jsonl.gz"
                self.hdfs.upload(source=self.serp_download_local_path, dest=target_hdfs_path, overwrite=True)
            else:
                print(f"[{datetime.now()}] error from upload_to_hdfs : ë¡œì»¬ì— í•´ë‹¹ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (serp_download_local_path : {self.serp_download_local_path})")
            if os.path.exists(self.serp_download_local_path_non_domestic):
                non_domestic_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_serp_non_domestic.jsonl.gz"
                self.hdfs.upload(source=self.serp_download_local_path_non_domestic, dest=non_domestic_hdfs_path, overwrite=True)
            else:
                print(f"[{datetime.now()}] error from upload_to_hdfs : ë¡œì»¬ì— í•´ë‹¹ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (serp_download_local_path_non_domestic : {self.serp_download_local_path_non_domestic})")
        except Exception as e:
            print(f"[{datetime.now()}] error from upload_to_hdfs : {e}")

    @error_notifier
    def count_line(self, path) -> int:
        if path.endswith(".gz"): # ì••ì¶• íŒŒì¼ì´ë©´ ì••ì¶•í•´ì œ
            read_path = GZipFileHandler.ungzip(path)
        else:
            read_path = path
        lines = JsonlFileHandler(read_path).count_line()
        if (path != read_path and 
            ~read_path.endswith(".gz")): # ì••ì¶• í’€ì—ˆë˜ íŒŒì¼ ë‹¤ì‹œ ì••ì¶•
            GZipFileHandler.gzip(read_path)
        return lines
    
    @error_notifier
    def extract_statistics(self):
        '''
        ìˆ˜ì§‘ ê²°ê³¼ì—ì„œ í†µê³„ ì •ë³´ ì¶”ì¶œ
        '''
        domestic_serp_count = self.count_line(self.serp_download_local_path)
        non_domestic_serp_count = self.count_line(self.serp_download_local_path_non_domestic)
        return {
            "total": domestic_serp_count + non_domestic_serp_count,
            "domestic": domestic_serp_count,
            "non_domestic": non_domestic_serp_count
        }

    def run(self):
        try:
            if self.log_task_history:
                self.task_history.set_task_start()
                self.task_history.set_task_in_progress()

            # íŒŒì¼ì´ ì¡´ì¬í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            print(f"[{datetime.now()}] í‚¤ì›Œë“œ íŒŒì¼ì˜ ìƒì„±ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘ì…ë‹ˆë‹¤: {self.new_trend_keyword_file}")
            while not os.path.exists(self.new_trend_keyword_file):
                time.sleep(60)  # 60ì´ˆë§ˆë‹¤ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸
            print(f"[{datetime.now()}] í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

            last_keyword_count = 0  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•œ í‚¤ì›Œë“œ ìˆ˜
            no_new_keywords_count = 0  # ìƒˆ í‚¤ì›Œë“œê°€ ì—†ì—ˆë˜ íšŸìˆ˜
            
            # ìƒˆ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²ƒì„ ëª‡ ë²ˆ í™•ì¸í• ì§€ ì„¤ì •
            if self.lang == "ja":
                max_no_new_keywords_count = 25
            else:
                max_no_new_keywords_count = 15
            
            already_collected_keywords = set() # ì´ë¯¸ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ
            if os.path.exists(self.serp_download_local_path): # ì„œí”„ ìˆ˜ì§‘í•œ ê²°ê³¼ ìˆìœ¼ë©´ ì¶”ê°€
                print(f"[{datetime.now()}] ê¸°ì¡´ ì„œí”„ ìˆ˜ì§‘ ì´ë ¥ì´ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì™„ë£Œëœ í‚¤ì›Œë“œ íŒŒì•…ì¤‘..")
                for serp in JsonlFileHandler(self.serp_download_local_path).read_generator():
                    already_collected_keywords.add(serp['search_parameters']['q'])
                print(f"[{datetime.now()}] ê¸°ì¡´ ì„œí”„ ìˆ˜ì§‘ ì™„ë£Œëœ í‚¤ì›Œë“œ íŒŒì•… ì™„ë£Œ : {len(already_collected_keywords)}ê°œ í‚¤ì›Œë“œ")
            while no_new_keywords_count < max_no_new_keywords_count or not os.path.exists(self.suggest_completed_file):
                # í‚¤ì›Œë“œë¥¼ ì½ìŒ
                trend_keywords = TXTFileHandler(self.new_trend_keyword_file).read_lines()
                
                # ìƒˆë¡œìš´ í‚¤ì›Œë“œ í™•ì¸
                if len(trend_keywords) > last_keyword_count:
                    print(f"[{datetime.now()}] ìƒˆë¡œìš´ í‚¤ì›Œë“œ ë°œê²¬! ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. (í‚¤ì›Œë“œ ê°œìˆ˜: {len(trend_keywords)})")
                    
                    # ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ì²˜ë¦¬
                    keywords_to_collect_serp = list(set(trend_keywords[last_keyword_count:]) - already_collected_keywords)
                    print(f"[{datetime.now()}] ğŸ§¹ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ì œê±° í›„ : ({len(keywords_to_collect_serp)})ê°œ")
                    keywords_to_collect_serp = list(set(keywords_to_collect_serp) - set(get_keywords_already_collected_serp(self.lang, self.job_id))) # ì˜¤ëŠ˜ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì œì™¸
                    print(f"[{datetime.now()}] ğŸ§¹ì˜¤ëŠ˜ ì´ë¯¸ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì œê±° í›„ : ({len(keywords_to_collect_serp)})ê°œ")
                    self.collect_serp(keywords_to_collect_serp) # ì´ë¯¸ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì œì™¸í•˜ê³  ìˆ˜ì§‘
                    self.append_keywords_to_serp_keywords_txt(keywords_to_collect_serp) # ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ hdfsì— ì €ì¥
                    already_collected_keywords = set(list(already_collected_keywords) + trend_keywords[last_keyword_count:])
                    
                    # ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•œ í‚¤ì›Œë“œ ìˆ˜ ì—…ë°ì´íŠ¸
                    last_keyword_count = len(trend_keywords)
                    no_new_keywords_count = 0  # ìƒˆ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ë¥¼ ì´ˆê¸°í™”
                    
                else:
                    # ìƒˆë¡œìš´ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¹´ìš´íŠ¸ë¥¼ ì¦ê°€
                    no_new_keywords_count += 1
                    print(f"[{datetime.now()}] ìƒˆë¡œìš´ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. {no_new_keywords_count}/{max_no_new_keywords_count} ë²ˆì§¸ ëŒ€ê¸° ì¤‘...")
                    
                # ì£¼ê¸°ì ìœ¼ë¡œ ëŒ€ê¸° (íŒŒì¼ì´ ë‹¤ì‹œ ì±„ì›Œì§ˆ ìˆ˜ ìˆë„ë¡ ëŒ€ê¸°)
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ íŒŒì¼ì„ í™•ì¸

                # ".gz" íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²´í¬ íšŒìˆ˜ë¥¼ ë„˜ê²¨ë„ ê³„ì† ëŒ€ê¸°
                if no_new_keywords_count >= max_no_new_keywords_count and not os.path.exists(self.suggest_completed_file):
                    no_new_keywords_count = max_no_new_keywords_count - 1  # ê³„ì† ëŒ€ê¸°í•˜ë„ë¡ ì¹´ìš´íŠ¸ë¥¼ ì¡°ì •
                    print(f"[{datetime.now()}] ìƒˆë¡œìš´ í‚¤ì›Œë“œê°€ ì—†ì§€ë§Œ {self.suggest_completed_file} íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•„ ëŒ€ê¸° ì¤‘... ({no_new_keywords_count}/{max_no_new_keywords_count})")

            print(f"[{datetime.now()}] ë” ì´ìƒ í‚¤ì›Œë“œê°€ ì¶”ê°€ë˜ì§€ ì•Šì•„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")

            # ì••ì¶•
            self.serp_download_local_path = GZipFileHandler.gzip(self.serp_download_local_path)
            self.serp_download_local_path_non_domestic = GZipFileHandler.gzip(self.serp_download_local_path_non_domestic)

            self.upload_to_hdfs()
            
        except Exception as e:
            print(f"[{datetime.now()}] ì„œí”„ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ\nError Msg : {e}")
            ds_trend_finder_dbgout_error(self.lang,
                                         f"{self.slack_prefix_msg}\nMessage : ì„œí”„ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ")
            if self.log_task_history:
                self.task_history.set_task_error(error_msg=str(e))
        else:
            print(f"[{datetime.now()}] ì„œí”„ ìˆ˜ì§‘ ì™„ë£Œ")
            statistic = self.extract_statistics()
            if self.log_task_history:
                self.task_history.set_task_completed(additional_info=statistic)
            ds_trend_finder_dbgout(self.lang,
                                   f"{self.slack_prefix_msg}\nMessage : ì„œí”„ ìˆ˜ì§‘ ì™„ë£Œ\nUpload Path : {self.hdfs_upload_folder}\nStatistics : (total: {statistic['total']} | domestic: {statistic['domestic']} | non_domestic: {statistic['non_domestic']})")
        
def find_last_job_id(suggest_type, lang, service, today):
    data_path = f"./data/result/{suggest_type}/{service}/{lang}"
    file_list = os.listdir(data_path)
    file_list.sort()
    last_file = file_list[-1]
    job_id = last_file[:10]
    return job_id

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lang", help="language", default=None)
    parser.add_argument("--service", help="service(google or youtube)", default=None)
    args = parser.parse_args()
    
    pid = os.getpid()
    print(f"pid : {pid}")

    suggest_type = "target"
    today = datetime.now().strftime("%Y%m%d")
    job_id = find_last_job_id(suggest_type, args.lang, args.service, today)
    print(f"job_id : {job_id}")
    
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì‹œì‘ ----------")
    entity_serp_daily = EntitySerpDaily(job_id, args.lang, args.service, log_task_history=True)
    entity_serp_daily.run()
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì™„ë£Œ ----------")