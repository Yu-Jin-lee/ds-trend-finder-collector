import os
import math
import argparse
from datetime import datetime, timedelta
from typing import List, Tuple

from collector.suggest_collector.suggest_collect import Suggest
from validator.trend_keyword_validator import is_trend_keyword
from utils.file import JsonlFileHandler, GZipFileHandler, TXTFileHandler, JsonFileHandler, has_file_extension
from utils.db import QueryDatabaseKo, QueryDatabaseJa, QueryDatabaseEn
from utils.text import extract_initial
from utils.data import combine_dictionary
from utils.hdfs import HdfsFileHandler
from utils.postgres import PostGres
from lang import Ko, Ja, En, filter_en_valid_trend_keyword, filter_en_valid_token_count
from config import postgres_db_config
from utils.decorator import error_notifier
from utils.task_history import TaskHistory
from utils.data import remove_duplicates_from_new_keywords
from validator.suggest_validator import SuggestValidator
from utils.text import extract_initial_next_target_keyword
from utils.converter import adjust_job_id
from utils.slack import ds_trend_finder_dbgout, ds_trend_finder_dbgout_error

def cnt_valid_suggest(suggestions:List[dict], 
                      target_keyword:str=None,
                      extension:str=None, # ì•ŒíŒŒë²³ í™•ì¥ ë¬¸ì (ìˆì„ ê²½ìš° ì…ë ¥, ì—†ì„ ê²½ìš°:None)
                      log:bool=False,
                      return_result:bool=False) -> int:
    try:
        cnt_valid = 0
        valid_suggest = []
        for suggestion in suggestions:
            if SuggestValidator.is_valid_suggest(suggestion['suggest_type'], suggestion['suggest_subtypes']):
                if (target_keyword != None and
                    extension != None): # íƒ€ê²Ÿ í‚¤ì›Œë“œì™€ í™•ì¥ ë¬¸ìê°€ ëª¨ë‘ ìˆì„ ê²½ìš°
                    initial_next_target_keyword = extract_initial_next_target_keyword([suggestion['text']], target_keyword=target_keyword)
                    if initial_next_target_keyword and len(initial_next_target_keyword) > 0:
                        if initial_next_target_keyword[0] == extension:
                            if log:
                                print(f"âœ”ï¸ {suggestion['text']} {suggestion['suggest_type']} {suggestion['suggest_subtypes']}")
                            cnt_valid += 1
                            valid_suggest.append(suggestion['text'])
                        else:
                            if log:
                                print(f"âŒâ— {suggestion['text']} {suggestion['suggest_type']} {suggestion['suggest_subtypes']}")
                    else:
                        if log:
                            print(f"âŒâ— {suggestion['text']} {suggestion['suggest_type']} {suggestion['suggest_subtypes']}")
                else:
                    if log:
                        print(f"âœ”ï¸ {suggestion['text']} {suggestion['suggest_type']} {suggestion['suggest_subtypes']}")
                    cnt_valid += 1
                    valid_suggest.append(suggestion['text'])
            else:
                if log:
                    print(f"âŒ {suggestion['text']} {suggestion['suggest_type']} {suggestion['suggest_subtypes']}")
        if return_result:
            return cnt_valid, valid_suggest
        return cnt_valid
    except Exception as e:
        if log:
            print(f"[{datetime.now()}] Error from cnt_valid_suggest: (target_keyword:{target_keyword}, extension:{extension}) | error msg : {e}")
        if return_result:
            return cnt_valid, valid_suggest
        return cnt_valid

class EntitySuggestDaily:
    def __init__(self, lang : str, service : str, job_id, log_task_history:bool=False):
        # ê¸°ë³¸ì •ë³´
        self.lang = lang
        self.service = service
        self.job_id = job_id
        self.project_name = "trend_finder"
        self.suggest_type = "target"
        self.task_name = f"ìˆ˜ì§‘-ì„œì œìŠ¤íŠ¸-{service}-{self.suggest_type}"
        self.target_letter_suggest_length = None

        # local ê´€ë ¨
        self.local_folder_path = f"./data/result/{self.suggest_type}/{self.service}/{self.lang}"
        if not os.path.exists(self.local_folder_path):
            os.makedirs(self.local_folder_path)
        self.trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords.txt"
        self.new_trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords_new.txt" # ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
        self.except_for_valid_trend_keywords_file = f"{self.local_folder_path}/{self.job_id}_except_for_valid_trend_keywords.txt" # ìœ íš¨í•˜ì§€ ì•Šì€ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
        self.local_result_path = f"{self.local_folder_path}/{self.job_id}.jsonl"
        self.trend_keyword_by_entity_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords_by_entity.json"
        
        # hdfs ê´€ë ¨
        self.hdfs = HdfsFileHandler()
        self.hdfs_upload_folder = f"/user/ds/wordpopcorn/{self.lang}/daily/{self.service}_suggest_for_llm_entity_topic/{self.job_id[:4]}/{self.job_id[:6]}/{self.job_id[:8]}/{self.job_id}"
        self.one_week_ago_trend_keywords = self.get_one_week_ago_trend_keywords(self.job_id[:8], lang) # ì´ì „ 7ì¼ê°„ ë‚˜ì™”ë˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        
        # Task History ê´€ë ¨
        self.log_task_history = log_task_history
        self.task_history = TaskHistory(postgres_db_config, "trend_finder", self.task_name, self.job_id, self.lang)
        
        # slack ê´€ë ¨
        self.slack_prefix_msg = f"Job Id : `{self.job_id}`\nTask Name : `{self.task_name}`-`{self.lang}`"

    @error_notifier
    def get_lang(self, lang:str):
        if lang == "ko":
            return Ko()
        if lang == "ja":
            return Ja()
        if lang == "en":
            return En()

    @error_notifier
    def get_llm_entity_topic(self) -> List[str]:
        '''
        get_llm_entity_topic ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        '''
        if self.lang == "ko":
            return QueryDatabaseKo.get_llm_entity_topic()
        if self.lang == "ja":
            return QueryDatabaseJa.get_llm_entity_topic()
        if self.lang == "en":
            return QueryDatabaseEn.get_llm_entity_topic()
        
    @error_notifier
    def get_already_collected_keywords(self) -> List[str]:
        already_collected_keywords = []
        if os.path.exists(self.local_result_path):
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ ì„œì œìŠ¤íŠ¸ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. (path : {self.local_result_path})")
            for line in JsonlFileHandler(self.local_result_path).read_generator():
                already_collected_keywords.append(line['keyword'])
            print(f"[{datetime.now()}] ã„´ {len(already_collected_keywords)}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘ë˜ì–´ ìˆìŒ")
        return list(set(already_collected_keywords))
        
    @error_notifier
    def get_extension(self) -> List[str]:
        '''
        ëŒ€ìƒ í‚¤ì›Œë“œ ìˆì„ ê²½ìš° í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        '''
        lang = self.get_lang(self.lang)
        return lang.suggest_extension_texts_by_rank(0) + lang.suggest_extension_texts_by_rank(1)

    @error_notifier
    def make_check_dict(self, lang:str) -> dict:
        '''
        ì´ˆì„±ë³„ ì™„ì„±í˜• ë¬¸ì ë”•ì…”ë„ˆë¦¬ ìƒì„±        
        '''
        if lang == "ko":
            letters = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
            # complete_letters = ['ì˜³', 'í ', 'ì• ', 'ë ¬', 'ì‹£', 'ìœ¤', 'ë±', 'íŠœ', 'í–¥', 'ë»”', 'í•„', 'ê²”', 'í•€', 'íŒ', 'ìœ', 'ë„˜', 'ì› ', 'ì‘¤', 'ì •', 'ìº‰', 'ëŠ”', 'ê¸ˆ', 'í–‰', 'ì¤Œ', 'ë¡±', 'í˜„', 'ì´ˆ', 'íŒ', 'ì´Œ', 'ë£¸', 'ëš«', 'ì”¹', 'ìŠˆ', 'í©', 'ìµ', 'ë†¨', 'ë´‰', 'íƒ±', 'ì¼œ', 'ë¹„', 'ì¼„', 'ë„·', 'ì—', 'ë¶™', 'ë´…', 'ê»˜', 'ë©•', 'í¬', 'ì°¼', 'ì¿„', 'íˆ', 'ë‹˜', 'ì¦‰', 'ì…¨', 'ë¥œ', 'ì„­', 'ìª½', 'ë‘ ', 'ì„°', 'ì»·', 'ë„›', 'ë§¤', 'ìš±', 'ìš°', 'ë¬˜', 'ë†€', 'ë”˜', 'ë§ˆ', 'ìˆ¯', 'ë‰˜', 'íŒ ', 'ì°©', 'êµ¿', 'í›”', 'ì•“', 'ë‚©', 'ì–‡', 'ì˜¨', 'ë…', 'ë©¸', 'ì•Œ', 'ì¬', 'ë‚¼', 'ê¶', 'ì»¬', 'íˆ­', 'ë°€', 'ì¸„', 'ë°', 'ê°–', 'ê³ ', 'ê¹Œ', 'í…Œ', 'ì¯¤', 'ë²™', 'ëƒ¥', 'êµ', 'ë©€', 'ê³', 'ì´', 'ì‰¬', 'í˜¸', 'ëˆ„', 'ì”©', 'ì—£', 'í’', 'ì›¬', 'ë£¹', 'ìŠ´', 'ë¯¸', 'ë¼', 'ì “', 'ë©ˆ', 'ìœ™', 'ëª¬', 'ë¹ˆ', 'ê¼­', 'ì„', 'ë¡¬', 'ìŠ', 'ì¶˜', 'í˜', 'í—Œ', 'ì›°', 'í€¸', 'ë’¤', 'ì†”', 'í›ˆ', 'ë™', 'í„', 'êº¼', 'ë ›', 'ë”±', 'ê¸´', 'ì‰´', 'ì¯”', 'ê¶¤', 'ë±…', 'ë£Œ', 'ëšœ', 'ìŠ¹', 'ë±€', 'êµ¶', 'ë¶', 'ëŠ', 'ì½°', 'ê±¸', 'ì˜†', 'ì–»', 'íƒ°', 'ìš•', 'ì½¥', 'íŒœ', 'ê³±', 'ì™”', 'ìº ', 'ê³°', 'í‹‹', 'ì¦', 'ì‰¼', 'ì™“', 'íƒ•', 'ì°”', 'ë°•', 'ìº”', 'ì¦˜', 'ì•¡', 'ë°±', 'ë‚³', 'ë½•', 'ëª©', 'í˜œ', 'ê²Œ', 'ê³¨', 'ë‹›', 'ë– ', 'ì›…', 'ë“­', 'ëœ', 'íƒˆ', 'ì˜Œ', 'ë‚Œ', 'ëŸ½', 'ëƒ‰', 'í¥', 'ì ', 'ëˆˆ', 'ê¸', 'í™', 'ê°¯', 'ì´', 'í‘œ', 'íŠ¹', 'íˆ¬', 'ê±·', 'í’‹', 'ë›´', 'í', 'ì§•', 'ë“', 'ë¶€', 'í‚·', 'ìˆ', 'íœ˜', 'ë¡€', 'í‘¼', 'ë¡œ', 'ë§¹', 'ì¦Œ', 'ê´´', 'ëœ', 'ë«', 'í´', 'í‚¹', 'ë¬¼', 'ì„¯', 'ë³', 'ë’·', 'ë§', 'ì°', 'ë‚«', 'ë§µ', 'ë ¤', 'ì²­', 'ê¸°', 'ëŒ', 'ì˜', 'ëœ°', 'ë©', 'ì‘', 'ë‹™', 'ì­', 'ìœ—', 'ë”§', 'ë‚„', 'ëŒ„', 'ëª¨', 'ì  ', 'ì°¢', 'ë”›', 'ë°˜', 'ì•„', 'í˜', 'ë²Œ', 'ë‹´', 'ì§œ', 'ë¨¹', 'í ', 'ì›¨', 'ë”œ', 'ë½', 'ë”', 'í™', 'êµ³', 'ì–¸', 'ì™¸', 'í•', 'ëœ»', 'ì•Š', 'ìš”', 'ì¼“', 'ë£°', 'ë°¸', 'ë‚¸', 'ì§‘', 'ë„¸', 'ë“¬', 'ë§‘', 'ë§·', 'ê°™', 'ì¹™', 'ë¤', 'í‚¬', 'ìœ', 'ë ™', 'íœ', 'ì§„', 'ë¿Œ', 'ë¥µ', 'ìŸ', 'ë‚´', 'í˜', 'ë ·', 'í„°', 'ì² ', 'í˜•', 'ì²˜', 'ì„¬', 'ë˜˜', 'ë³‘', 'ë¦¬', 'ì½§', 'ì', 'ì„', 'ê²', 'ì ¸', 'ë¹Œ', 'ë§º', 'ì°¸', 'ë¹…', 'ë©§', 'ì°»', 'ì”', 'ë•…', 'ë µ', 'ë¦°', 'ì¶¤', 'ì–¹', 'íƒ', 'ê²ƒ', 'ë‚˜', 'ìŠ¤', 'ë¢°', 'ì·¨', 'ê°ˆ', 'ì„', 'ì£ ', 'ë¸', 'ì—­', 'ë”¥', 'ë“œ', 'ëª½', 'ë·°', 'í‡´', 'ë‚€', 'ë²¤', 'í…œ', 'í”½', 'ìº„', 'í”ˆ', 'ì†', 'í—¤', 'ë”°', 'ë›¸', 'ë§', 'ë…', 'ê±´', 'ëº', 'í˜', 'ì¨Œ', 'ì‹', 'ë¯€', 'ë“ ', 'ë“€', 'êµ°', 'ì„', 'ëº€', 'ì¡¸', 'ìˆ™', 'ì©”', 'ë§™', 'ì‚´', 'ê· ', 'ìº£', 'ì§–', 'ê´œ', 'ì´‰', 'ê²', 'ë—', 'ê·¸', 'ë‹¬', 'ìƒµ', 'ì§¬', 'ë² ', 'ë”ª', 'ì„¸', 'ì¶œ', 'ë ', 'ëƒ„', 'ëœ', 'ëˆ', 'ìƒ¹', 'í•™', 'ë€', 'ëš', 'ëŠ', 'í‘¹', 'ëœ¬', 'ëª¹', 'ë²”', 'íƒ­', 'ì‰°', 'ìƒ', 'ê¸', 'ë¯¼', 'ë»—', 'ì¹­', 'ì¿µ', 'íœ ', 'ê±±', 'ë‚±', 'ë„ˆ', 'ë‹‰', 'ì‰½', 'ëˆ', 'ì´˜', 'ìŠµ', 'ì˜', 'ê¹”', 'í…¨', 'ê¸¸', 'í”¼', 'ë§Œ', 'ì¹«', 'ë·”', 'ì™„', 'ì¨', 'ë³¶', 'ì‚¶', 'ì±Œ', 'ì‹¬', 'ìˆ­', 'í‹ˆ', 'ì»¸', 'ë¦¼', 'í¼', 'ê·€', 'ì¥”', 'ê¹Š', 'ë¦‡', 'ê»', 'í……', 'ë”©', 'ë®¬', 'ì¤€', 'ë‚ ', 'ì”Œ', 'íš¡', 'ë¤˜', 'ì•½', 'ì—…', 'ì‹¤', 'ì½”', 'ë¶„', 'ë©”', 'ê¶Œ', 'ë¦­', 'ì½¤', 'í”„', 'ë¹¨', 'ë‹¤', 'ì»´', 'ì–µ', 'ê²¨', 'ì‡¼', 'í‹°', 'ì¸¡', 'ì•…', 'ë°›', 'ì—˜', 'í‰', 'ë¡', 'ë£»', 'ì¿¨', 'íŒ¸', 'ëŒ€', 'ë¶“', 'ëŒ“', 'ìš©', 'ì—´', 'ìµœ', 'íˆ¼', 'ì½', 'í€„', 'íŒŒ', 'ëŸ´', 'ì§“', 'ì˜€', 'ë°–', 'ê¹¥', 'ìƒˆ', 'ì“´', 'ë¹µ', 'ì‚­', 'ì¹©', 'ì‚°', 'ìš´', 'ë³„', 'ê·¹', 'ì½˜', 'ë‚¬', 'ë ¹', 'ì œ', 'ëŒˆ', 'ë•', 'ì›¹', 'í˜‘', 'ë°', 'ë¿”', 'í‘', 'ê²¬', 'ëœ¸', 'ëˆŒ', 'ë¦´', 'ê°', 'ì›Œ', 'ë€Œ', 'ì˜', 'í…”', 'ë¥˜', 'ë‘', 'ì', 'ë½', 'í™', 'ì²¨', 'ì…œ', 'ì©', 'ë±‰', 'ë¬´', 'ë£¡', 'ë­‰', 'ëŸ¿', 'ì˜µ', 'ë‹·', 'ê¶ˆ', 'êµ½', 'ê¹€', 'ë²¼', 'ë´', 'ì–€', 'ë¨¸', 'ì²™', 'ë¹”', 'ë‡¨', 'ë§¡', 'ë¶', 'ì ', 'ë¾°', 'ê¿‹', 'ë„', 'ë¦„', 'ë¡¯', 'ì§¸', 'ì§ˆ', 'ë¼', 'ëŠ¥', 'ë´¤', 'í›¤', 'ê¾€', 'ì‡', 'ë”', 'íœ©', 'ë©°', 'ë­', 'ë¸”', 'ì ', 'ë”', 'í’€', 'íƒ€', 'ì‹­', 'ì§', 'ëŒ', 'ëº¨', 'ë–¨', 'ë§¨', 'í‹¸', 'í¸', 'ì¹¨', 'ë¤¼', 'ì˜', 'íƒ¤', 'ì¡Œ', 'ê¹', 'ë¡­', 'ìœ„', 'ë¬¸', 'ê³³', 'ìˆ ', 'ê¹¬', 'ìœ¡', 'íƒ', 'ì”»', 'í•´', 'ì¤„', 'ë¹™', 'ì¹´', 'ì¦', 'ì¸', 'ë', 'í†¤', 'ë…„', 'ì•±', 'ëŠ˜', 'í†µ', 'ë Œ', 'í€˜', 'ì§±', 'ë˜‘', 'ì„ ', 'ê³§', 'ì ¤', 'íŒ', 'ìŠ', 'ë¥¸', 'ìœ ', 'ì›”', 'ë©˜', 'ì±™', 'ë“±', 'íŒ', 'ë €', 'ì¥', 'í‰', 'ì „', 'ë‘”', 'ë³´', 'ë¹¡', 'í„', 'ì‡ ', 'ë£¬', 'ëŠª', 'ì¡±', 'ë‘˜', 'ë ‰', 'ëŒ', 'í—ˆ', 'ë­', 'ë„', 'ì«“', 'ì¹µ', 'ì”', 'ë', 'ì–´', 'ì¡', 'ë‚¡', 'ë°´', 'ë‹¦', 'ì¹œ', 'ë®', 'ë©', 'ì‚½', 'ì„¤', 'ë†’', 'ì¿ ', 'ë‹', 'ì”½', 'ëœ¯', 'ê² ', 'ê¿€', 'í—›', 'ì…‹', 'ì›', 'íŒ”', 'ë„¤', 'ë¸', 'ë§', 'ë¼', 'í‰', 'ì†', 'ì§§', 'ê²¸', 'ë„¨', 'ê¶', 'ì…¸', 'ê¹ƒ', 'ë˜', 'ë¯¿', 'í„¸', 'ì–', 'ê³„', 'ìƒ›', 'í­', 'ì–˜', 'ë¥­', 'í•œ', 'ë“¤', 'ê²ª', 'ì‹¶', 'ì¬', 'ì—¬', 'ë ', 'ì§¤', 'ì“°', 'ê¹', 'í€´', 'í˜', 'ì„¼', 'ê¸€', 'ëˆ', 'ì³¤', 'ë ¨', 'ë„¥', 'ë€', 'ë¡¤', 'ì„', 'ë°', 'ì°Œ', 'ì‚¼', 'í†¡', 'ì½•', 'ì»¤', 'í•', 'ëª«', 'ë˜¥', 'ë’€', 'ë¨¼', 'ë‚¨', 'í°', 'í›Œ', 'ê»‘', 'êµ¬', 'ê³¤', 'ê¸‰', 'ìœµ', 'ì”¨', 'í•µ', 'ìƒ·', 'ì•', 'ë¹›', 'ì¦ˆ', 'ìŒ', 'ê²¼', 'ë•', 'ë…€', 'ì…°', 'ì–„', 'ì˜®', 'ê°±', 'ë‘‘', 'ì°¨', 'ë¥¨', 'ì´¬', 'ìˆ²', 'ê¶‚', 'ë‹®', 'ìƒ‰', 'íŒ¡', 'ê³¡', 'ê°“', 'ìŒˆ', 'ë‹Œ', 'ì˜›', 'í¼', 'ë›°', 'ê¼½', 'ëŸ¬', 'í•­', 'ì•°', 'í†°', 'ì ', 'ì¹¼', 'ìº˜', 'ì—‰', 'ì¢…', 'ì¿¼', 'ìˆ±', 'ë²š', 'ë˜', 'ì¼€', 'ë£½', 'ì›', 'ìƒ€', 'ì°', 'ë–»', 'í˜”', 'ë‘¥', 'ì·„', 'íˆ°', 'ë”•', 'íƒœ', 'ì¢€', 'í™œ', 'í™€', 'ìƒ¤', 'ì™œ', 'íŒ', 'ë¥¼', 'íŒ', 'í', 'ë°‘', 'ì•˜', 'í…€', 'í•©', 'ëª‡', 'ë¤„', 'ë…•', 'í—', 'ë ¥', 'ë“ˆ', 'í—¨', 'ê´€', 'í‘¸', 'ì»µ', 'í•¨', 'í„±', 'ë®¤', 'ë•', 'ì¼ˆ', 'ì£¼', 'ë‚™', 'íŠ€', 'ì¤¬', 'ì½©', 'ë»‘', 'ë¸Œ', 'ì—Œ', 'ë¿', 'ë ', 'ë­˜', 'ê¾¸', 'í‹´', 'ë¬¶', 'í””', 'íšŸ', 'ì—‘', 'ì…', 'ì„€', 'í‚¥', 'ì•¨', 'ì˜¤', 'ìƒ', 'ì¸ ', 'ë†“', 'ë§›', 'ë§', 'ì¶§', 'ì—½', 'ëª…', 'ì‰', 'ì­‰', 'ì…”', 'ì§ ', 'ë†', 'í‚´', 'ì£', 'ì…€', 'ì°®', 'ì½œ', 'ì‘', 'ëŠ„', 'ë²¨', 'ë†ˆ', 'ì„±', 'ìƒ', 'ì•¼', 'ê³¼', 'ëŠ‘', 'ë ', 'íƒ„', 'ëŸ­', 'ì¡´', 'ë€”', 'í‚¤', 'ë„', 'ì‹¼', 'ë­„', 'ë„Œ', 'ê°‡', 'í—˜', 'ì™•', 'ê½ƒ', 'ì¥', 'ë²—', 'ì˜¬', 'ëˆ ', 'ìƒ', 'ë¦½', 'ëŒ', 'ì†Œ', 'ë§', 'ë­‡', 'ë£¨', 'ì£½', 'ì¼', 'ì…˜', 'ê¿‡', 'ì‰', 'ì¦', 'ì –', 'ë–¼', 'ë„£', 'ê¹¡', 'ì²´', 'ì¹˜', 'ë§´', 'ì†¥', 'ë§‰', 'íŠ¸', 'ë', 'í†±', 'í™‰', 'ë“', 'ë””', 'ì°¾', 'ê½¤', 'ê±€', 'ì—°', 'ë“£', 'í‹±', 'ë²„', 'ë¦¿', 'ê°€', 'ë•¡', 'ì˜', 'í™˜', 'íœ´', 'í¬', 'ê·¤', 'ë¡ ', 'ë³¸', 'í—', 'ê³¶', 'íŠ¼', 'ê²©', 'ì¦™', 'í“¨', 'ë…', 'êµ´', 'ì‹«', 'íŒ€', 'ì—”', 'ë¥ ', 'íŒ»', 'ê½‚', 'ëŸ‰', 'ìˆ˜', 'ë´', 'íƒ‘', 'í†¨', 'ë²•', 'ë¶ˆ', 'ë‡Œ', 'ë²³', 'ê²¹', 'ê¹', 'ìº', 'ì»¨', 'ì¨', 'ê¼¬', 'ì£„', 'íšŒ', 'ì™ ', 'ë‚š', 'ë ', 'í–‡', 'ì•‰', 'ì˜¹', 'í™•', 'ì„œ', 'ë™', 'ê±°', 'ë‘', 'ì‡„', 'ë°¤', 'ëˆ”', 'ë‰´', 'ë‹«', 'ëš±', 'íƒ', 'ë…¹', 'í…ƒ', 'ëª°', 'ì²¼', 'í‘', 'ë°Œ', 'ì Š', 'ëª„', 'íƒ¬', 'í”', 'ê¿”', 'í“°', 'ë ˜', 'í˜', 'ì¸µ', 'í™', 'ëŒ', 'ë‘¡', 'ìª¼', 'ì³', 'ë´', 'ë°°', 'ì¿¤', 'íŒ…', 'ë¥™', 'ë§', 'ì°¬', 'ì•—', 'ë‹¹', 'ë–´', 'íƒ”', 'ìŒ“', 'ì„£', 'ì— ', 'ê°', 'ê°š', 'ì«„', 'íŒŸ', 'í˜€', 'ë‹', 'ê¹…', 'ë“¯', 'ë–¡', 'ëƒˆ', 'ê´‘', 'ì†œ', 'ìƒ˜', 'í€', 'ê¸‹', 'ë˜', 'íŠ¿', 'ì–¼', 'íš¨', 'ë²…', 'ê½‰', 'ëµ', 'ìŠ¬', 'ë‹ˆ', 'ì€', 'í…', 'í«', 'ìŠ›', 'ëƒ…', 'ìœˆ', 'ì¤˜', 'ë ‡', 'ì¹', 'íŒ°', 'ëœ¨', 'ì‚¿', 'í…', 'ë ¸', 'ê¾¼', 'ì˜´', 'ì‹ ', 'ì••', 'êµ­', 'ì¾Œ', 'ë¶‰', 'ìš¸', 'ë¼ˆ', 'í™”', 'íŒ¥', 'í’ˆ', 'ê°•', 'ë°©', 'ë„“', 'ë•„', 'í• ', 'ìŠ˜', 'ë³•', 'ë‘¬', 'ë®Œ', 'ì™', 'í°', 'ë¿', 'í‹€', 'ì–‘', 'ë§', 'ë©œ', 'ë¥', 'ìƒ´', 'ë–¤', 'ë—', 'ì¶©', 'ë‹­', 'ì´›', 'ê²€', 'ê¿¨', 'ê°„', 'íŒ', 'ë²ˆ', 'íŒ½', 'ì—¿', 'ë½€', 'ë¬', 'í–ˆ', 'ì…ˆ', 'ì‚', 'ì¢‹', 'ë¥´', 'ì°œ', 'ì‹±', 'êº¾', 'ë‚¯', 'ë³µ', 'ì¿¡', 'ì˜', 'ìŠ¨', 'ì”€', 'ì—¼', 'ì©', 'í–„', 'ì—®', 'ì±ˆ', 'í‰ˆ', 'ë', 'ì™¼', 'ë‹¨', 'ì„¹', 'ë¦', 'ê³½', 'ì ˆ', 'ì¥¬', 'ê°¤', 'ë‚­', 'ì¼', 'íƒ“', 'ì¶°', 'ì ‘', 'ë¹—', 'í˜¼', 'ì§š', 'ì†¡', 'ì©¡', 'ìˆœ', 'ê¼´', 'ë°”', 'ëœ©', 'ëˆ´', 'ì¤', 'ë„¬', 'í†ˆ', 'ë±ƒ', 'ê»', 'ë•€', 'ì¢Œ', 'ëŒ', 'ê´Œ', 'ì ', 'ì—ˆ', 'ì‹¹', 'í¼', 'ë¶•', 'ì—†', 'í°', 'ì”¬', 'ë”¸', 'ìŒ', 'ë‚®', 'ë°­', 'ëŸ', 'í¼', 'íŒ¨', 'ì™ˆ', 'ì›ƒ', 'ì—‡', 'í´', 'ê¼ˆ', 'í', 'êµ‰', 'ë”¤', 'ê³µ', 'ê·œ', 'í•¸', 'ì°°', 'ë¤', 'ëƒ', 'ì•™', 'ë¦‰', 'í™©', 'ëŠ¦', 'íˆ´', 'ê·¼', 'ì±…', 'ë½', 'ì±”', 'ì—', 'ìœŒ', 'ë„', 'ë ´', 'ì¢', 'í—¬', 'ë³€', 'ë­”', 'ë²½', 'ìƒŒ', 'ê°’', 'ì§', 'í›¨', 'í„´', 'ì‹œ', 'ë¨', 'ê¼¼', 'ëª»', 'ëŠ ', 'ë¨', 'ìœ¼', 'ë´„', 'í™ˆ', 'ë¹½', 'ëŸ°', 'ë”´', 'ë„‰', 'ì—„', 'ë…¼', 'ë°¥', 'ë´‡', 'ë¬»', 'ì¸', 'í•‘', 'ë©‹', 'í…', 'ì•µ', 'ëŒ”', 'ë”', 'ì•¤', 'ìº¡', 'ëŠ¬', 'í˜¹', 'ë¦…', 'ë‰œ', 'í›¼', 'ë°‹', 'ê¿ˆ', 'ë©', 'ê²°', 'ìŸ', 'ì²©', 'í', 'ë»', 'ì§€', 'íƒ ', 'ì†Ÿ', 'í´', 'ì¼°', 'ë§¥', 'í', 'ìˆ', 'ë£©', 'êµµ', 'ê°”', 'ì¼', 'ë–³', 'ë¹¼', 'ì¤‘', 'ì˜¥', 'ì“¸', 'í‚¨', 'ê°‘', 'ë…”', 'ì¹ ', 'í•«', 'ì²œ', 'ë©', 'í•˜', 'ë‹', 'ë³¼', 'ì˜·', 'í¬', 'ëŸ¼', 'ì', 'íŠ¬', 'ê°', 'ë…˜', 'ë©´', 'ê¹œ', 'íŒ¬', 'ë ˆ', 'ë•Œ', 'ëª¸', 'ìŠ·', 'ì•ˆ', 'ì±„', 'ê»´', 'ë¯¹', 'ë°Ÿ', 'ë²¡', 'ë…¸', 'ë¹š', 'í›„', 'ë„', 'í›—', 'ê°œ', 'ë°œ', 'ì–½', 'ë‚œ', 'ì•”', 'ì‹¸', 'ì¡°', 'ë½‘', 'í† ', 'ì…‰', 'ì§', 'ì˜ˆ', 'ê´„', 'ê²‰', 'ê¿°', 'ë†”', 'íˆ', 'ì¶•', 'ê²½', 'ì·Œ', 'ì', 'ì§™', 'ì²¸', 'ì™€', 'ì €', 'ì¹¸', 'ìŒ€', 'í˜ˆ', 'íŒ©', 'ë¿œ', 'ì©Œ', 'ë—', 'ë©¤', 'ë‹¿', 'ë¬µ', 'ë‹¥', 'ì¶”', 'ì‘¥', 'í”Œ', 'ì¼', 'ìˆ¨', 'ê½', 'ë‹µ', 'ì°½', 'ìœ¨', 'ë˜', 'ì›€', 'ì»«', 'ë°', 'ì²«', 'ê¹¨', 'ëŠ™', 'ì‚¬', 'ìˆ«', 'í¡', 'ë§˜', 'ë¹ ', 'íš']
            complete_letters = ['ê°€', 'ê°œ', 'ê±°', 'ê²Œ', 'ê²¨', 'ê³„', 'ê³ ', 'ê³¼', 'ê´´', 'êµ', 'êµ¬', 'ê¶ˆ', 'ê¶¤', 'ê·€', 'ê·œ', 'ê·¸', 'ê¸°', 'ê¹Œ', 'ê¹¨', 'êº¼', 'ê»˜', 'ê»´', 'ê¼¬', 'ê½¤', 'ê¾€', 'ê¾¸', 'ê¿”', 'ê¿°', 'ë€Œ', 'ë„', 'ë¼', 'ë‚˜', 'ë‚´', 'ëƒ', 'ë„ˆ', 'ë„¤', 'ë…€', 'ë…¸', 'ë†”', 'ë‡Œ', 'ë‡¨', 'ëˆ„', 'ëˆ ', 'ë‰˜', 'ë‰´', 'ëŠ', 'ëŠ¬', 'ë‹ˆ', 'ë‹¤', 'ëŒ€', 'ë”', 'ë°', 'ëŒ', 'ë„', 'ë¼', 'ë˜', 'ë‘', 'ë‘¬', 'ë’¤', 'ë“€', 'ë“œ', 'ë””', 'ë”°', 'ë•Œ', 'ë– ', 'ë–¼', 'ë˜', 'ëšœ', 'ë›°', 'ëœ¨', 'ë„', 'ë ', 'ë¼', 'ë˜', 'ë´', 'ëŸ¬', 'ë ˆ', 'ë ¤', 'ë¡€', 'ë¡œ', 'ë¢°', 'ë£Œ', 'ë£¨', 'ë¤„', 'ë¤¼', 'ë¥˜', 'ë¥´', 'ë¦¬', 'ë§ˆ', 'ë§¤', 'ë¨¸', 'ë©”', 'ë©°', 'ëª¨', 'ë¬˜', 'ë¬´', 'ë­', 'ë®¤', 'ë¯€', 'ë¯¸', 'ë°”', 'ë°°', 'ë²„', 'ë² ', 'ë²¼', 'ë³´', 'ë´', 'ë¶€', 'ë·”', 'ë·°', 'ë¸Œ', 'ë¹„', 'ë¹ ', 'ë¹¼', 'ë»', 'ë¼ˆ', 'ë½€', 'ë¾°', 'ë¿Œ', 'ì˜', 'ì‚', 'ì‚¬', 'ìƒˆ', 'ìƒ¤', 'ì„€', 'ì„œ', 'ì„¸', 'ì…”', 'ì…°', 'ì†Œ', 'ì‡„', 'ì‡ ', 'ì‡¼', 'ìˆ˜', 'ì‰', 'ì‰¬', 'ìŠˆ', 'ìŠ¤', 'ì‹œ', 'ì‹¸', 'ì¨', 'ì„', 'ì˜', 'ì', 'ì‘¤', 'ì“°', 'ì”Œ', 'ì”¨', 'ì•„', 'ì• ', 'ì•¼', 'ì–˜', 'ì–´', 'ì—', 'ì—¬', 'ì˜ˆ', 'ì˜¤', 'ì™€', 'ì™œ', 'ì™¸', 'ìš”', 'ìš°', 'ì›Œ', 'ì›¨', 'ìœ„', 'ìœ ', 'ìœ¼', 'ì˜', 'ì´', 'ì', 'ì¬', 'ì €', 'ì œ', 'ì ¸', 'ì¡°', 'ì¢Œ', 'ì£„', 'ì£ ', 'ì£¼', 'ì¤˜', 'ì¥', 'ì¥¬', 'ì¦ˆ', 'ì§€', 'ì§œ', 'ì§¸', 'ì©Œ', 'ìª¼', 'ì¯”', 'ì°Œ', 'ì°¨', 'ì±„', 'ì²˜', 'ì²´', 'ì³', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶°', 'ì·Œ', 'ì·¨', 'ì¸„', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ìº', 'ì»¤', 'ì¼€', 'ì¼œ', 'ì½”', 'ì½°', 'ì¾Œ', 'ì¿„', 'ì¿ ', 'ì¿¼', 'í€˜', 'í€´', 'í', 'í¬', 'í‚¤', 'íƒ€', 'íƒœ', 'í„°', 'í…Œ', 'í…¨', 'í† ', 'í‡´', 'íˆ¬', 'í‰ˆ', 'íŠ€', 'íŠœ', 'íŠ¸', 'í‹°', 'íŒŒ', 'íŒ¨', 'í¼', 'í˜', 'í´', 'í', 'í¬', 'í‘œ', 'í‘¸', 'í“¨', 'í”„', 'í”¼', 'í•˜', 'í•´', 'í—ˆ', 'í—¤', 'í˜€', 'í˜œ', 'í˜¸', 'í™”', 'íšŒ', 'íš¨', 'í›„', 'í›¼', 'íœ˜', 'íœ´', 'í', 'í¬', 'íˆ']
            check_dict = {let:[] for let in letters}
            for let in complete_letters:
                check_dict[extract_initial(let)].append(let)
            return check_dict

        elif lang == "ja":
            letters = ['ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ã', 'ã', 'ã‘', 'ã“', 'ã•', 'ã—', 'ã™', 'ã›', 'ã', 'ãŸ', 'ã¡', 'ã¤', 'ã¦', 'ã¨', 'ãª', 'ã«', 'ã¬', 'ã­', 'ã®', 'ã¯', 'ã²', 'ãµ', 'ã¸', 'ã»', 'ã¾', 'ã¿', 'ã‚€', 'ã‚', 'ã‚‚', 'ã‚„', 'ã‚†', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚', 'ã‚’', 'ã‚“']
            check_dict = {let:[] for let in letters}
            for x in letters:
                for y in letters:
                    check_dict[x].append(x+y)
            return check_dict
        
        elif lang == "en":
            alphabets = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']#, ' ']
            check_dict = {alphabet:[] for alphabet in alphabets}
            for x in alphabets:
                if x == " ": continue
                for y in alphabets:
                    check_dict[x].append(x+y)
            return check_dict
        
        else:
            print(f"[{datetime.now()}] {lang}ì˜ check_dictëŠ” ì—†ìŠµë‹ˆë‹¤.")

    @error_notifier
    def get_suggest_and_request_serp(self,
                                     targets : List[str],
                                     result_file_path : str, # "*.jsonl"
                                     num_processes:int
                                     ) -> str: # ì €ì¥ ê²½ë¡œ ë°˜í™˜
        '''
        ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ìš”ì²­ ë° ë¡œì»¬ì— ì €ì¥ + ì„œí”„ ìˆ˜ì§‘ ìš”ì²­
        '''
        suggest = Suggest()
        batch_size = 10000
        print(f"[{datetime.now()}] ìˆ˜ì§‘í•  ê°œìˆ˜ : {len(targets)} | batch_size : {batch_size}")
        for i in range(0, len(targets), batch_size):
            start = datetime.now()
            result = suggest._requests(targets[i : i+batch_size], 
                                       self.lang, 
                                       self.service, 
                                       num_processes = num_processes)
            print(f"[{datetime.now()}]    ã„´ batch {int((i+batch_size)/batch_size)}/{math.ceil(len(targets)/batch_size)} finish : {datetime.now()-start}")
            JsonlFileHandler(result_file_path).write(result)
            # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                trend_keywords = [suggestion['text'] for res in result for suggestion in res['suggestions'] if is_trend_keyword(suggestion['text'], # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
                                                                                                                        suggestion['suggest_type'], 
                                                                                                                        suggestion['suggest_subtypes'])]
                valid_trend_keywords = [keyword for keyword in trend_keywords if self.filter_valid_trend_keywords(keyword)] # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¤‘ ìœ íš¨í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
                print(f"[{datetime.now()}]       ã„´âœ”ï¸ìœ íš¨í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(valid_trend_keywords)}/{len(trend_keywords)}")
                TXTFileHandler(self.trend_keyword_file).write(valid_trend_keywords) # valid_trend_keywords ì €ì¥
                TXTFileHandler(self.except_for_valid_trend_keywords_file).write(list(set(trend_keywords) - set(valid_trend_keywords))) # valid_trend_keywordsë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì €ì¥
                # ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
                new_trend_keywords = list(remove_duplicates_from_new_keywords(set(self.one_week_ago_trend_keywords), set(valid_trend_keywords)))
                TXTFileHandler(self.new_trend_keyword_file).write(new_trend_keywords)
            except Exception as e:
                print(f"[{datetime.now()}] íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥ ì‹¤íŒ¨ : {e}")
            # TODO : ì„œí”„ ìˆ˜ì§‘ ìš”ì²­ (kafka)
            
        return result_file_path
    
    @error_notifier
    def load_keywords_from_hdfs(self, file_path):
        """HDFSì—ì„œ txt íŒŒì¼ì„ ì½ì–´ì™€ì„œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        try:
            contents = self.hdfs.load(file_path)
            keywords = set([line.strip() for line in contents.splitlines() if line.strip()])  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            return sorted(keywords)
        except Exception as e:
            print(f"[{datetime.now()}] HDFSì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return []
        
    @error_notifier
    def get_all_txt_files(self, date_folder_path) -> List[str]:
        '''
        ì…ë ¥í•œ date_folder_path í•˜ìœ„ ê²½ë¡œë¥¼ ëŒë©´ì„œ .txt íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        '''
        all_txt_files = []

        if not self.hdfs.exist(date_folder_path):
            print(f"[{datetime.now()}] {date_folder_path} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return all_txt_files
        
        # í˜„ì¬ í´ë”ì˜ í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜´
        job_id_dirs = [d for d in self.hdfs.list(date_folder_path) if not has_file_extension(d)] # ë””ë ‰í† ë¦¬ë§Œ ê°€ì ¸ì˜´

        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ìˆœíšŒ
        for job_id in job_id_dirs:
            job_id_path = f"{date_folder_path}/{job_id}"
            if not self.hdfs.exist(job_id_path):
                continue
            files = self.hdfs.list(job_id_path)
            for file in files:
                if file.endswith("_trend_keywords.txt"):
                    all_txt_files.append(f"{job_id_path}/{file}")

        return all_txt_files

    @error_notifier
    def get_one_week_ago_trend_keywords(self, today, lang):
        '''
        ì´ì „ 7ì¼ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        '''
        services = ['google', 'youtube']
        
        try:
            today_datetime = datetime.strptime(today, "%Y%m%d")

            one_week_ago_trend_keywords = []
            for i in range(1, 8, 1):
                date = today_datetime - timedelta(days=i)
                date = date.strftime("%Y%m%d")
                for service in services:
                    txt_files = self.get_all_txt_files(f"/user/ds/wordpopcorn/{lang}/daily/{service}_suggest_for_llm_entity_topic/{date[:4]}/{date[:6]}/{date[:8]}")
                    for file in txt_files:
                        one_week_ago_trend_keywords += self.load_keywords_from_hdfs(file)

            one_week_ago_trend_keywords = list(set(one_week_ago_trend_keywords))
            print(f"í‚¤ì›Œë“œ ê°œìˆ˜ : {len(set(one_week_ago_trend_keywords))}")

            return one_week_ago_trend_keywords
        except Exception as e:
            print(f"[{datetime.now()}] ì´ì „ 7ì¼ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ : {e}")
            return []

    @error_notifier
    def read_already_collected_text(self):
        '''
        ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ì½ê¸°
        '''
        collected_texts = []
        if os.path.exists(self.local_result_path):
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤! ({self.local_result_path})")
            for line in JsonlFileHandler(self.local_result_path).read_generator():
                collected_texts.append(line['keyword'])
            collected_texts = list(set(collected_texts))
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ : {len(collected_texts)}ê°œ")
        else:
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (not found file {self.local_result_path})")
        return collected_texts
    
    @error_notifier
    def get_target_letter_suggest(self, llm_entity_topic:List[str]):
        '''
        ëŒ€ìƒ í‚¤ì›Œë“œ ìˆëŠ” ê²½ìš° í•´ë‹¹ í‚¤ì›Œë“œì˜ 0, 1ë‹¨ê³„ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        '''
        try:
            target_num_process = 95
            print(f"[{datetime.now()}] ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ : {target_num_process}")
            lang = self.get_lang(self.lang)
            extension_texts = lang.suggest_extension_texts_by_rank(0) + lang.suggest_extension_texts_by_rank(1) # í™•ì¥ í…ìŠ¤íŠ¸ 1ê¸€ìì„
            if self.lang == "ja": # ì¼ë³¸ì˜ ê²½ìš° ë„ì–´ì“°ê¸° í•˜ì§€ ì•ŠìŒ
                targets = [topic + t for topic in llm_entity_topic for t in extension_texts] # ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            else:
                targets = [topic + " " + t for topic in llm_entity_topic for t in extension_texts] # ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

            print(f"[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œ 0, 1 ë‹¨ê³„ extension text ì¶”ê°€ í›„ ê°œìˆ˜ {len(targets)}")
            self.target_letter_suggest_length = len(targets) # ëŒ€ìƒ í‚¤ì›Œë“œ 0, 1ë‹¨ê³„ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘í•  ê°œìˆ˜
            print(f"[{datetime.now()}] self.target_letter_suggest_length : {self.target_letter_suggest_length}")
            already_collected_texts = self.read_already_collected_text() # ì´ë¯¸ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì½ê¸°
            targets = list(set(targets) - set(already_collected_texts))
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ì œì™¸í•œ ê°œìˆ˜ {len(targets)}")
            already_collected_keywords = self.get_already_collected_keywords()
            targets = list(set(targets) - set(already_collected_keywords))
            self.get_suggest_and_request_serp(targets, self.local_result_path, num_processes=target_num_process)
            print(f"[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ 0, 1 ë‹¨ê³„ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"[{datetime.now()}] ERROR from get_target_letter_suggest : {e}")
    
    @error_notifier
    def get_target_charactor_suggest(self, llm_entity_topic:List[str]):
        '''
        ëŒ€ìƒ í‚¤ì›Œë“œ ìˆëŠ” ê²½ìš° í•´ë‹¹ í‚¤ì›Œë“œì˜ ì™„ì„±í˜• ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        í•œêµ­ì˜ ê²½ìš° ì´ˆì„±ì˜ valid í•œ ì„œì œìŠ¤íŠ¸ê°€ valid_thresholdê°œ ì´ìƒì´ë¼ë©´ í•´ë‹¹ ì´ˆì„±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì™„ì„±í˜• ë¬¸ìì˜ ì„œì œìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
        ì¼ë³¸ì˜ ê²½ìš° 
        '''
        try:
            valid_threshold = 8
            target_num_process = 95            
            print(f"[{datetime.now()}] ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ : {target_num_process}")
            check_dict = combine_dictionary([self.make_check_dict("ko"), self.make_check_dict("ja"), self.make_check_dict("en")])
            targets = []
            cnt = 0
            for line in JsonlFileHandler(self.local_result_path).read_generator(line_len = self.target_letter_suggest_length): # ëŒ€ìƒ í‚¤ì›Œë“œì˜ 0, 1 ë‹¨ê³„ë§Œ ìˆ˜ì§‘ëœ ìƒíƒœ (get_target_letter_suggestì˜ ê²°ê³¼)
                cnt += 1
                extension_letter = line['keyword'][-1] # í™•ì¥ ë¬¸ì
                target_keyword = line['keyword'][:-1].strip() # ëŒ€ìƒ í‚¤ì›Œë“œ
                if (target_keyword in llm_entity_topic and
                    extension_letter in check_dict): # í•´ë‹¹ ë¬¸ìê°€ ì´ˆì„±ì¸ ê²½ìš°
                    # print(f"\n[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œ : {target_keyword} | í™•ì¥ ë¬¸ì : {extension_letter}")
                    if cnt_valid_suggest(line['suggestions'], 
                                            target_keyword=target_keyword, 
                                            extension=extension_letter, 
                                            log=False) >= valid_threshold: # validí•œ ì„œì œìŠ¤íŠ¸ê°€ valid_thresholdê°œ ì´ìƒì´ë©´
                        # print(f"=>ğŸ˜€'{extension_letter}'ì˜ validí•œ ì„œì œìŠ¤íŠ¸ ê°œìˆ˜ê°€ {valid_threshold}ê°œ ì´ìƒì…ë‹ˆë‹¤.\n")
                        # í•´ë‹¹ ì´ˆì„±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì™„ì„±í˜• ë¬¸ìì˜ ì„œì œìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                        extension_texts = list(set(check_dict[extension_letter]))
                        extension_texts = [t for t in extension_texts if t != ""]
                        if self.lang == "ja":
                            targets += [target_keyword + t for t in extension_texts]
                        else:
                            targets += [target_keyword + " " + t for t in extension_texts]
            targets = list(set(targets))
            print(f"[{datetime.now()}] {self.local_result_path}ì—ì„œ {cnt}ì¤„ ì½ìŒ (self.target_letter_suggest_length : {self.target_letter_suggest_length})")
            print(f"[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œ extension text ì¶”ê°€ í›„ ê°œìˆ˜ {len(targets)}")
            already_collected_texts = self.read_already_collected_text()
            targets = list(set(targets) - set(already_collected_texts))
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ì œì™¸í•œ ê°œìˆ˜ {len(targets)}")
            already_collected_keywords = self.get_already_collected_keywords()
            targets = list(set(targets) - set(already_collected_keywords))
            self.get_suggest_and_request_serp(targets, self.local_result_path, num_processes=target_num_process)
        except Exception as e:
            print(f"[{datetime.now()}] ERROR from get_target_charactor_suggest : {e}")
        else:
            print(f"[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œì˜ ì™„ì„±í˜• ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")

    @error_notifier
    def filter_valid_trend_keywords(self, trend_keyword:str) -> bool:
        '''
        ì…ë ¥ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ìœ íš¨í•œ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
        '''
        if self.lang == "en":
            if filter_en_valid_trend_keyword(trend_keyword) & filter_en_valid_token_count(trend_keyword): return True
            else: return False
        else:
            return True
    
    @error_notifier
    def run_google(self):
        '''
        êµ¬ê¸€ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        '''
        # ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘       
        try:
            # ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘í•  entity topic ê°€ì ¸ì˜¤ê¸° (from DB)
            llm_entity_topic = self.get_llm_entity_topic()

            # 1. ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
            print(f"[{datetime.now()}] ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘ (ìˆ˜ì§‘í•  entity topic ê°œìˆ˜ : {len(llm_entity_topic)})")
            self.get_target_letter_suggest(llm_entity_topic) # ëŒ€ìƒ í‚¤ì›Œë“œ + 0, 1ë‹¨ê³„ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
            self.get_target_charactor_suggest(llm_entity_topic) # ëŒ€ìƒ í‚¤ì›Œë“œ + ì™„ì„±í˜•, ì•ŒíŒŒë²³ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
            # ì••ì¶•
            self.local_result_path = GZipFileHandler.gzip(self.local_result_path)
        except Exception as e:
            print(f"[{datetime.now()}] {self.lang} {self.service} ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ : {e}")
        else:
            print(f"[{datetime.now()}] {self.lang} {self.service} ëŒ€ìƒ í‚¤ì›Œë“œ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")

    @error_notifier
    def count_trend_keyword(self) -> int:
        try:
            trend_keywords = TXTFileHandler(self.trend_keyword_file).read_lines()
            trend_keywords = list(set(trend_keywords))
            print(f"[{datetime.now()}] {self.lang} {self.service} íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(trend_keywords)}")
            return len(trend_keywords)
        except Exception as e:
            print(f"[{datetime.now()}] Error from count_trend_keyword | {self.lang} {self.service} íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨ | {e}")

    @error_notifier
    def upload_to_hdfs(self):
        target_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}.jsonl.gz"
        self.hdfs.upload(source=self.local_result_path, dest=target_hdfs_path, overwrite=True)

        trend_keyword_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_trend_keywords.txt"
        self.hdfs.upload(source=self.trend_keyword_file, dest=trend_keyword_hdfs_path, overwrite=True)
        
        new_trend_keyword_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_trend_keywords_new.txt"
        self.hdfs.upload(source=self.new_trend_keyword_file, dest=new_trend_keyword_hdfs_path, overwrite=True)

    @error_notifier
    def extract_trend_keywords_by_entity(self):
        '''
        entityë³„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
        '''
        entities = self.get_llm_entity_topic()
        trend_keywords_by_entity = {}
        if self.local_result_path.endswith(".gz"):
            self.local_result_path = GZipFileHandler.ungzip(self.local_result_path)
        for line in JsonlFileHandler(self.local_result_path).read_generator(): 
            for suggestion in line['suggestions']:
                s_type = suggestion['suggest_type']
                s_subtypes = suggestion['suggest_subtypes']
                keyword = line['keyword']
                entity = ' '.join(keyword.split(' ')[:-1])
                if entity not in entities:
                    print(f"[{datetime.now()}] {entity}ëŠ” ëŒ€ìƒ entityê°€ ì•„ë‹™ë‹ˆë‹¤.")
                if entity not in trend_keywords_by_entity: # ë”•ì…”ë„ˆë¦¬ì— entityê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                    trend_keywords_by_entity[entity] = {}
                if keyword not in trend_keywords_by_entity[entity]: # ë”•ì…”ë„ˆë¦¬ì— keywordê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                    trend_keywords_by_entity[entity][keyword] = []
                if is_trend_keyword(suggestion['text'], s_type, s_subtypes):
                    trend_keywords_by_entity[entity][keyword].append(suggestion['text'])
        JsonFileHandler(self.trend_keyword_by_entity_file).write(trend_keywords_by_entity)
        self.local_result_path = GZipFileHandler.gzip(self.local_result_path)

    @error_notifier
    def update_task_history(self, history:List[Tuple]):
        try:
            PostGres.insert_to_task_history(history, "update")
        except Exception as e:
            print(f"[{datetime.now()}] failed to update task history: {e}")

    def run(self):
        try:
            start_time = datetime.now()
            print(f"job_id : {self.job_id}")
            if self.log_task_history:
                self.task_history.set_task_start()
                self.task_history.set_task_in_progress()
                
            if self.service == "google":
                self.run_google()

            self.count_trend_keyword()

            self.upload_to_hdfs()
            
            if self.log_task_history:
                self.task_history.set_task_completed()
            end_time = datetime.now()
        except Exception as e:
            print(f"[{datetime.now()}] ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ\nError Msg : {e}")
            ds_trend_finder_dbgout_error(f"{self.slack_prefix_msg}\nMessage : ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ")
            if self.log_task_history:
                self.task_history.set_task_error(error_msg=e)
        else:
            print(f"[{datetime.now()}] ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
            ds_trend_finder_dbgout(f"{self.slack_prefix_msg}\nMessage : ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ\nUpload Path : {self.hdfs_upload_folder}\n{end_time-start_time} ì†Œìš”")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lang", help="language", default=None)
    parser.add_argument("--service", help="service(google or youtube)", default=None)
    args = parser.parse_args()
    
    pid = os.getpid()
    print(f"pid : {pid}")

    # job_id ìƒì„±
    if args.lang == "en": # ë¯¸êµ­ì¼ ê²½ìš° í˜„ì¬ ì‹œê°„ì—ì„œ 14ì‹œê°„ ì´ì „ìœ¼ë¡œ job_id ì¡°ì •
        job_id = adjust_job_id(datetime.now().strftime("%Y%m%d%H"), 14)
    else:
        job_id = datetime.now().strftime("%Y%m%d%H")
    print(f"job_id : {job_id}")

    # ìˆ˜ì§‘ ì‹œì‘
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì‹œì‘ ----------")
    entity_daily = EntitySuggestDaily(args.lang, args.service, job_id, log_task_history=True)
    entity_daily.run()
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì™„ë£Œ ----------")