import os
import gc
import math
import argparse
from datetime import datetime, timedelta
from typing import List

from collector.suggest_collector.suggest_collect import Suggest
from validator.trend_keyword_validator import is_trend_keyword, cnt_valid_suggest
from utils.file import JsonlFileHandler, GZipFileHandler, TXTFileHandler, has_file_extension
from utils.data import Trie, remove_duplicates_from_new_keywords, remove_duplicates_from_new_keywords_ko, remove_duplicates_with_spaces
from utils.hdfs import HdfsFileHandler
from utils.task_history import TaskHistory
from utils.slack import ds_trend_finder_dbgout, ds_trend_finder_dbgout_error
from utils.decorator import error_notifier
from utils.converter import adjust_job_id
from lang import Ko, Ja, En, filter_en_valid_trend_keyword, filter_en_valid_token_count
from lang.ja.ja import hiragana, katakana, katakana_chouon, youon, sokuon_hiragana, sokuon_katakana, gairaigo_katakana, alphabet, number, kanji
            
from config import postgres_db_config

class EntitySuggestDaily:
    def __init__(self, lang : str, service : str, job_id:str, log_task_history:bool=False):
        # ê¸°ë³¸ì •ë³´
        self.lang = lang
        self.service = service
        self.project_name = "trend_finder"
        self.suggest_type = "basic"
        self.job_id = job_id

        # local ê´€ë ¨
        self.local_folder_path = f"./data/result/{self.suggest_type}/{self.service}/{self.lang}"
        if not os.path.exists(self.local_folder_path):
            os.makedirs(self.local_folder_path)
        self.trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords.txt"
        self.new_trend_keyword_file = f"{self.local_folder_path}/{self.job_id}_trend_keywords_new.txt" # ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
        self.except_for_valid_trend_keywords_file = f"{self.local_folder_path}/{self.job_id}_except_for_valid_trend_keywords.txt" # ìœ íš¨í•˜ì§€ ì•Šì€ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
        self.local_result_path = f"{self.local_folder_path}/{self.job_id}.jsonl"
        
        # hdfs ê´€ë ¨
        self.hdfs = HdfsFileHandler()
        self.hdfs_upload_folder = f"/user/ds/wordpopcorn/{self.lang}/daily/{self.service}_suggest_for_llm_entity_topic/{self.job_id[:4]}/{self.job_id[:6]}/{self.job_id[:8]}/{self.job_id}"
        self.past_trend_keywords : List[str] = self.get_past_trend_keywords(self.job_id[:8], lang, 28) # ì´ì „ Nì¼ì „ ë‚˜ì™”ë˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        
        # Task History ê´€ë ¨
        self.task_name = f"ìˆ˜ì§‘-ì„œì œìŠ¤íŠ¸-{service}-{self.suggest_type}"
        self.log_task_history = log_task_history
        self.task_history_updater = TaskHistory(postgres_db_config, self.project_name, self.task_name, self.job_id, self.lang)

        # slack ê´€ë ¨
        self.slack_prefix_msg = f"Job Id : `{self.job_id}`\nTask Name : `{self.task_name}`-`{self.lang}`"

        # í†µê³„ëŸ‰ ê´€ë ¨
        self.statistics = {"call": {}, "valid": {}, "trend_keyword": {}}

    @error_notifier
    def get_lang(self, lang:str):
        if lang == "ko":
            return Ko()
        if lang == "ja":
            return Ja()
        if lang == "en":
            return En()
        else:
            print(f"[{datetime.now()}] {lang} í•´ë‹¹ êµ­ê°€ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (EntitySuggestDaily)")
            raise ValueError(f"{lang} í•´ë‹¹ êµ­ê°€ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (EntitySuggestDaily)")
    
    @error_notifier
    def load_keywords_from_hdfs(self, file_path):
        """HDFSì—ì„œ txt íŒŒì¼ì„ ì½ì–´ì™€ì„œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        contents = self.hdfs.load(file_path)
        keywords = set([line.strip() for line in contents.splitlines() if line.strip()])  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return sorted(keywords)
    
    @error_notifier
    def get_all_txt_files(self, date_folder_path):
        '''
        ì…ë ¥í•œ date_folder_path í•˜ìœ„ ê²½ë¡œë¥¼ ëŒë©´ì„œ .txt íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        '''
        if not self.hdfs.exist(date_folder_path):
            print(f"[{datetime.now()}] message from get_all_txt_files : {date_folder_path} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return []
        # í˜„ì¬ í´ë”ì˜ í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜´
        job_id_dirs = [d for d in self.hdfs.list(date_folder_path) if not has_file_extension(d)] # ë””ë ‰í† ë¦¬ë§Œ ê°€ì ¸ì˜´
        all_txt_files = []

        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ìˆœíšŒ
        for job_id in job_id_dirs:
            job_id_path = f"{date_folder_path}/{job_id}"
            files = self.hdfs.list(job_id_path)
            for file in files:
                if file.endswith("_trend_keywords.txt"):
                    all_txt_files.append(f"{job_id_path}/{file}")

        return all_txt_files
    
    @error_notifier
    def get_already_collected_keywords(self) -> List[str]:
        already_collected_keywords = []
        if os.path.exists(self.local_result_path):
            print(f"[{datetime.now()}] ì´ë¯¸ ìˆ˜ì§‘ëœ ì„œì œìŠ¤íŠ¸ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. (path : {self.local_result_path})")
            for line in JsonlFileHandler(self.local_result_path).read_generator():
                already_collected_keywords.append(line['keyword'])
            print(f"[{datetime.now()}] ã„´ {len(already_collected_keywords)}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘ë˜ì–´ ìˆìŒ")
        return list(set(already_collected_keywords))
    
    @error_notifier
    def get_past_trend_keywords(self, today:str, lang:str, days:int) -> List[str]:
        '''
        ì´ì „ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        '''
        services = ['google', 'youtube']
                
        print(f"[{datetime.now()}] {today} ê¸°ì¤€ ì´ì „ {days}ì¼ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°")
        today_datetime = datetime.strptime(today, "%Y%m%d")
        past_trend_keywords = []
        for i in range(1, days+1, 1):
            date = today_datetime - timedelta(days=i)
            date = date.strftime("%Y%m%d")
            for service in services:
                txt_files = self.get_all_txt_files(f"/user/ds/wordpopcorn/{lang}/daily/{service}_suggest_for_llm_entity_topic/{date[:4]}/{date[:6]}/{date[:8]}")
                for file in txt_files:
                    past_trend_keywords += self.load_keywords_from_hdfs(file)

        past_trend_keywords = list(set(past_trend_keywords))
        print(f"í‚¤ì›Œë“œ ê°œìˆ˜ : {len(set(past_trend_keywords))}")

        return past_trend_keywords
    
    @error_notifier  
    def get_suggest_and_request_serp(self,
                                     targets : List[str],
                                     result_file_path : str, # "*.jsonl"
                                     num_processes:int
                                     ) -> str: # ì €ì¥ ê²½ë¡œ ë°˜í™˜
        '''
        ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ìš”ì²­ ë° ë¡œì»¬ì— ì €ì¥ + ì„œí”„ ìˆ˜ì§‘ ìš”ì²­
        '''
        suggest = Suggest()
        batch_size = 10000
        print(f"[{datetime.now()}] ìˆ˜ì§‘í•  ê°œìˆ˜ : {len(targets)} | batch_size : {batch_size}")
        for i in range(0, len(targets), batch_size):
            start = datetime.now()
            result = suggest._requests(targets[i : i+batch_size], 
                                       self.lang, 
                                       self.service, 
                                       num_processes = num_processes)
            print(f"[{datetime.now()}]    ã„´ batch {int((i+batch_size)/batch_size)}/{math.ceil(len(targets)/batch_size)} finish : {datetime.now()-start}")
            JsonlFileHandler(result_file_path).write(result)
            # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                trend_keywords = [suggestion['text'] for res in result for suggestion in res['suggestions'] if is_trend_keyword(suggestion['text'], # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
                                                                                                                        suggestion['suggest_type'], 
                                                                                                                        suggestion['suggest_subtypes'])]
                valid_trend_keywords = [keyword for keyword in trend_keywords if self.filter_valid_trend_keywords(keyword)] # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¤‘ ìœ íš¨í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
                print(f"[{datetime.now()}]       ã„´âœ”ï¸ìœ íš¨í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(valid_trend_keywords)}/{len(trend_keywords)}")
                TXTFileHandler(self.trend_keyword_file).write(valid_trend_keywords) # valid_trend_keywords ì €ì¥
                TXTFileHandler(self.except_for_valid_trend_keywords_file).write(list(set(trend_keywords) - set(valid_trend_keywords))) # valid_trend_keywordsë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì €ì¥
                # ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if self.lang == "ko":
                    new_trend_keywords = list(remove_duplicates_from_new_keywords_ko(set(self.past_trend_keywords), set(valid_trend_keywords)))
                else:
                    new_trend_keywords = list(remove_duplicates_from_new_keywords(set(self.past_trend_keywords), set(valid_trend_keywords)))
                TXTFileHandler(self.new_trend_keyword_file).write(new_trend_keywords)
            except Exception as e:
                print(f"[{datetime.now()}] íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥ ì‹¤íŒ¨ : {e}")
            # TODO : ì„œí”„ ìˆ˜ì§‘ ìš”ì²­ (kafka)
            
        return result_file_path

    @error_notifier
    def filter_valid_trend_keywords(self, trend_keyword:str) -> bool:
        '''
        ì…ë ¥ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ìœ íš¨í•œ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
        '''
        if self.lang == "en":
            if filter_en_valid_trend_keyword(trend_keyword) & filter_en_valid_token_count(trend_keyword): return True
            else: return False
        else:
            return True
    
    @error_notifier
    def get_1st_extension(self) -> List[str]:
        '''
        1ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        '''
        extension_rank_1 = []
        if self.lang == "ja":
            # extension_rank_1 = hiragana + katakana + katakana_chouon + youon + sokuon_hiragana + sokuon_katakana + gairaigo_katakana + alphabet + number + kanji
            extension_rank_1 = hiragana + katakana + katakana_chouon + alphabet + number + kanji

        print(f"[{datetime.now()}] 1ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°œìˆ˜ : {len(extension_rank_1)}")
        return extension_rank_1
    
    @error_notifier
    def get_2st_extension(self) -> List[str]:
        '''
        2ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        '''
        extension_rank_2 = []
        if self.lang == "ja":
            # extension_rank_2 = hiragana + katakana + katakana_chouon + youon + sokuon_hiragana + sokuon_katakana + gairaigo_katakana + alphabet + number
            extension_rank_2 = hiragana + katakana + katakana_chouon + alphabet + number

        print(f"[{datetime.now()}] 2ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°œìˆ˜ : {len(extension_rank_2)}")
        return extension_rank_2
    
    @error_notifier
    def get_3st_extension(self) -> List[str]:
        '''
        3ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        '''
        extension_rank_3 = []
        if self.lang == "ja":
            # extension_rank_3 = hiragana + katakana + katakana_chouon + youon + sokuon_hiragana + sokuon_katakana + gairaigo_katakana + alphabet + number
            extension_rank_3 = hiragana + katakana + katakana_chouon + alphabet + number

        print(f"[{datetime.now()}] 3ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°œìˆ˜ : {len(extension_rank_3)}")
        return extension_rank_3
    
    @error_notifier
    def filtering_valid_trend_keywords(self,
                                       targets:List[str],
                                       valid_threshold:int) -> List[str]:
        '''
        ìœ íš¨í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
        '''
        valid_targets = []
        start_time = datetime.now()
        for line in JsonlFileHandler(self.local_result_path).read_generator():
            keyword = line['keyword']
            if keyword in targets:
                valid_suggest_cnt, valid_suggests = cnt_valid_suggest(suggestions=line['suggestions'],
                                                                      input_text=keyword,
                                                                      return_result=True)
                if valid_suggest_cnt >= valid_threshold:
                    valid_targets.append(keyword)
        print(f"[{datetime.now()}] ìœ íš¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì†Œìš” ì‹œê°„ : {datetime.now()-start_time}")
        return valid_targets
        
    @error_notifier
    def run_basic(self):
        '''
        ê¸°ë³¸ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        '''
        basic_num_process = 100
        print(f"[{datetime.now()}] job_id : {self.job_id} | service : {self.service} â­ê¸°ë³¸ ì„œì œìŠ¤íŠ¸â­ ìˆ˜ì§‘ ì‹œì‘")

        # 1ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        print(f"[{datetime.now()}] ğŸ“ 1ë‹¨ê³„")
        targets_1 = self.get_1st_extension()
        # ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        print(f"[{datetime.now()}] âœ… 1ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì‘ (ì´ ìˆ˜ì§‘í•  ê°œìˆ˜ : {len(targets_1)}, process_num : {basic_num_process})")
        targets = targets_1
        self.statistics["call"]["rank1"] = len(targets)
        self.get_suggest_and_request_serp(targets, self.local_result_path, num_processes=basic_num_process)

        valid_targets_1 = self.filtering_valid_trend_keywords(targets_1, 6)
        print(f"âœ… 1ë‹¨ê³„ ìœ íš¨í•œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(valid_targets_1)}/{len(targets_1)}")

        # 2ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        print(f"[{datetime.now()}] ğŸ“ 2ë‹¨ê³„")
        extension_2 = self.get_2st_extension()
        targets_2 = [x + y for x in valid_targets_1 for y in extension_2]
        # ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        print(f"[{datetime.now()}] âœ… 2ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì‘ (ì´ ìˆ˜ì§‘í•  ê°œìˆ˜ : {len(targets_2)}, process_num : {basic_num_process})")
        already_collected_keywords = self.get_already_collected_keywords() # ì´ë¯¸ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ëª©ë¡
        targets = list(set(targets_2) - set(already_collected_keywords))
        self.statistics["call"]["rank2"] = len(targets)
        self.get_suggest_and_request_serp(targets, self.local_result_path, num_processes=basic_num_process)

        # 2ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        valid_targets_2 = self.filtering_valid_trend_keywords(targets_2, 6)
        print(f"âœ… 2ë‹¨ê³„ ìœ íš¨í•œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(valid_targets_2)}/{len(targets_2)}")

        # 3ë‹¨ê³„ í™•ì¥ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        print(f"[{datetime.now()}] ğŸ“ 3ë‹¨ê³„")
        extension_3 = self.get_3st_extension()
        targets_3 = [x + y for x in valid_targets_2 for y in extension_3]
        # ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘
        print(f"[{datetime.now()}] âœ… 3ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì‘ (ì´ ìˆ˜ì§‘í•  ê°œìˆ˜ : {len(targets_3)}, process_num : {basic_num_process})")
        already_collected_keywords = self.get_already_collected_keywords() # ì´ë¯¸ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ëª©ë¡
        targets = list(set(targets_3) - set(already_collected_keywords))
        self.statistics["call"]["rank3"] = len(targets)
        self.get_suggest_and_request_serp(targets, self.local_result_path, num_processes=basic_num_process)

        self.local_result_path = GZipFileHandler.gzip(self.local_result_path)
        print(f"[{datetime.now()}] ğŸ‰ ê¸°ë³¸ ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")

    @error_notifier
    def count_trend_keyword(self) -> dict:
        # ì´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜
        trend_keywords = TXTFileHandler(self.trend_keyword_file).read_lines()
        trend_keywords = remove_duplicates_with_spaces(trend_keywords)
        print(f"[{datetime.now()}] {self.lang} {self.service} íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(trend_keywords)}")
        # ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜
        new_trend_keywords = TXTFileHandler(self.new_trend_keyword_file).read_lines()
        new_trend_keywords = remove_duplicates_with_spaces(new_trend_keywords)
        print(f"[{datetime.now()}] {self.lang} {self.service} ìƒˆë¡œ ë‚˜ì˜¨ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°œìˆ˜ : {len(new_trend_keywords)}")
        return {"total": len(trend_keywords),
                "new": len(new_trend_keywords)}

    @error_notifier
    def upload_to_hdfs(self):
        basic_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}.jsonl.gz"
        self.hdfs.upload(source=self.local_result_path, dest=basic_hdfs_path, overwrite=True)

        trend_keyword_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_trend_keywords.txt"
        self.hdfs.upload(source=self.trend_keyword_file, dest=trend_keyword_hdfs_path, overwrite=True)

        new_trend_keyword_hdfs_path = f"{self.hdfs_upload_folder}/{self.job_id}_{self.suggest_type}_trend_keywords_new.txt"
        self.hdfs.upload(source=self.new_trend_keyword_file, dest=new_trend_keyword_hdfs_path, overwrite=True)

    def run(self):
        try:
            start_time = datetime.now()
            if self.log_task_history:
                self.task_history_updater.set_task_start()
                self.task_history_updater.set_task_in_progress()

            self.run_basic()

            trend_keyword_cnt = self.count_trend_keyword()
            self.statistics["trend_keyword"] = trend_keyword_cnt

            self.upload_to_hdfs()
            
            if self.log_task_history:
                self.task_history_updater.set_task_completed(additional_info=self.statistics)
            end_time = datetime.now()
        except Exception as e:
            print(f"[{datetime.now()}] ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ\nError Msg : {e}")
            error_msg = (
                f"{self.slack_prefix_msg}\n"
                f"Message: ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‘ì—… ì¢…ë£Œ\n"
                f"Error: {e}"
            )
            ds_trend_finder_dbgout_error(self.lang, error_msg)
            if self.log_task_history:
                self.task_history_updater.set_task_error(error_msg=str(e))
        else:
            print(f"[{datetime.now()}] ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
            print(f"[Statistics]\n{self.statistics}")
            success_msg = (
                            f"{self.slack_prefix_msg}\n"
                            f"Message: ì„œì œìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ\n"
                            f"Upload Path: {self.hdfs_upload_folder}\n"
                            f"Processing Time: {end_time-start_time} ì†Œìš”\n"
                            f"Statistics: (total: {trend_keyword_cnt['total']} | new: {trend_keyword_cnt['new']})\n"
                        )
            ds_trend_finder_dbgout(self.lang, success_msg)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lang", help="language", default=None)
    parser.add_argument("--service", help="service(google or youtube)", default=None)
    args = parser.parse_args()
    
    pid = os.getgid()
    print(f"pid : {pid}")

    # job_id ìƒì„±
    if args.lang == "en": # ë¯¸êµ­ì¼ ê²½ìš° í˜„ì¬ ì‹œê°„ì—ì„œ 14ì‹œê°„ ì´ì „ìœ¼ë¡œ job_id ì¡°ì •
        job_id = adjust_job_id(datetime.now().strftime("%Y%m%d%H"), 14)
    else:
        job_id = datetime.now().strftime("%Y%m%d%H")

    print(f"job_id : {job_id}")

    # ìˆ˜ì§‘ ì‹œì‘
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì‹œì‘ ----------")
    entity_daily = EntitySuggestDaily(args.lang, args.service, job_id, log_task_history=True)
    entity_daily.run()
    print(f"---------- [{datetime.now()}] {args.lang} {args.service} ìˆ˜ì§‘ ì™„ë£Œ ----------")